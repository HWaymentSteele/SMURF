{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('af_backprop')\n",
    "from utils import set_dropout, pdb_to_string, jnp_rmsd, save_pdb\n",
    "\n",
    "import laxy\n",
    "import sw_functions as sw\n",
    "import network_functions as nf\n",
    "\n",
    "# import libraries\n",
    "from alphafold.common import protein\n",
    "from alphafold.data import pipeline, templates\n",
    "from alphafold.model import data, config, model\n",
    "from alphafold.data import parsers\n",
    "\n",
    "from alphafold.common import residue_constants\n",
    "data_alphabet = list(\"ARNDCQEGHILKMFPSTWYVX-\")\n",
    "idx2aa = {n:a for n,a in enumerate(data_alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat(filename, alphabet=\"ARNDCQEGHILKMFPSTWYV\"):\n",
    "  '''\n",
    "  Given A3M file (from hhblits)\n",
    "  return MSA (aligned), MS (unaligned) and ALN (alignment)\n",
    "  '''\n",
    "  def parse_fasta(filename):\n",
    "    '''function to parse fasta file'''    \n",
    "    header, sequence = [],[]\n",
    "    lines = open(filename, \"r\")\n",
    "    for line in lines:\n",
    "      line = line.rstrip()\n",
    "      if len(line) == 0: pass\n",
    "      else:\n",
    "        if line[0] == \">\":\n",
    "          header.append(line[1:])\n",
    "          sequence.append([])\n",
    "        else:\n",
    "          sequence[-1].append(line)\n",
    "    lines.close()\n",
    "    sequence = [''.join(seq) for seq in sequence]\n",
    "    return header, sequence\n",
    "\n",
    "  names, seqs = parse_fasta(filename)  \n",
    "  a2n = {a:n for n,a in enumerate(alphabet)}\n",
    "  def get_seqref(x):\n",
    "    n,seq,ref,aligned_seq = 0,[],[],[]\n",
    "    for aa in list(x):\n",
    "      if aa != \"-\":\n",
    "        seq.append(a2n.get(aa.upper(),-1))\n",
    "        if aa.islower(): ref.append(-1); n -= 1\n",
    "        else: ref.append(n); aligned_seq.append(seq[-1])\n",
    "      else: aligned_seq.append(-1)\n",
    "      n += 1\n",
    "    return seq, ref, aligned_seq\n",
    "  \n",
    "  # get the multiple sequence alignment\n",
    "  max_len = 0\n",
    "  ms, aln, msa = [],[],[]\n",
    "  for seq in seqs:\n",
    "    seq_,ref_,aligned_seq_ = get_seqref(seq)\n",
    "    if len(seq_) > max_len: max_len = len(seq_)\n",
    "    ms.append(seq_)\n",
    "    msa.append(aligned_seq_)\n",
    "    aln.append(ref_)\n",
    "  \n",
    "  return msa, ms, aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lddt_loss(logits):\n",
    "  num_bins = logits.shape[-1]\n",
    "  bin_width = 1.0 / num_bins\n",
    "  bin_centers = jnp.arange(start=0.5 * bin_width, stop=1.0, step=bin_width)\n",
    "  probs = jax.nn.softmax(logits, axis=-1)\n",
    "  predicted_lddt_ca = jnp.sum(probs * bin_centers[None, :], axis=-1)\n",
    "  return 1 - predicted_lddt_ca.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(DOM=\"T1064-D1\"):\n",
    "  a3m_file = f\"casp/{DOM}.mmseqs.id90cov75.a3m\"\n",
    "  _, ms, aln = get_feat(a3m_file)\n",
    "  a = {\"ms\":ms,\"aln\":aln}\n",
    "  a_ = {\"ms\":[],\"aln\":[]}\n",
    "  lens = []\n",
    "  for aln_,ms_ in zip(a[\"aln\"],a[\"ms\"]):\n",
    "    a_[\"ms\"].append(ms_)\n",
    "    a_[\"aln\"].append(aln_)\n",
    "    lens.append(len(ms_))\n",
    "\n",
    "  lens = np.asarray(lens)\n",
    "  ms = nf.one_hot(nf.pad_max(a_[\"ms\"]))\n",
    "  aln = nf.one_hot(nf.pad_max(a_[\"aln\"]))\n",
    "  N = len(ms)\n",
    "\n",
    "  protein_obj = protein.from_pdb_string(pdb_to_string(f\"casp_/{DOM}.pdb\"))\n",
    "  msa, mtx = parsers.parse_a3m(open(a3m_file,\"r\").read())\n",
    "  feature_dict = {\n",
    "      **pipeline.make_sequence_features(sequence=msa[0],description=\"none\",num_res=len(msa[0])),\n",
    "      **pipeline.make_msa_features(msas=[msa], deletion_matrices=[mtx])\n",
    "  }\n",
    "  feature_dict[\"residue_index\"] = protein_obj.residue_index\n",
    "  \n",
    "  return {\"lens\":lens,\n",
    "          \"ms\":ms,\n",
    "          \"aln\":aln,\n",
    "          \"feature_dict\":feature_dict,\n",
    "          \"N\":N,\n",
    "          \"protein_obj\":protein_obj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOM = \"T1070-D1\"\n",
    "INPUTS = prep_inputs(DOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(INPUTS[\"feature_dict\"][\"residue_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup which model params to use\n",
    "model_name = \"model_3_ptm\"\n",
    "model_config = config.model_config(model_name)\n",
    "model_config.model.global_config.use_remat = True # checkpointing (to reduce memory usage)\n",
    "\n",
    "model_config.model.num_recycle = 3\n",
    "model_config.data.common.num_recycle = 3\n",
    "\n",
    "model_config.data.eval.max_msa_clusters = INPUTS[\"N\"]\n",
    "model_config.data.common.max_extra_msa = 1\n",
    "model_config.data.eval.masked_msa_replace_fraction = 0\n",
    "\n",
    "# backprop through recycles\n",
    "model_config.model.backprop_recycle = False\n",
    "model_config.model.embeddings_and_evoformer.backprop_dgram = False\n",
    "\n",
    "model_config = set_dropout(model_config,0)\n",
    "\n",
    "# setup model\n",
    "model_params = data.get_model_haiku_params(model_name=model_name, data_dir=\".\")\n",
    "model_runner = model.RunModel(model_config, model_params, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup inputs\n",
    "inputs = model_runner.process_features(INPUTS[\"feature_dict\"], random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_fn(model_runner, x_ref_len, supervised=False):\n",
    "  def mod(msa_params, key, inputs, model_params, msa_inputs):  \n",
    "\n",
    "    # get embedding per sequence\n",
    "    emb = nf.Conv1D_custom(msa_params[\"emb\"])(msa_inputs[\"x\"],key=key,scale=0.1)\n",
    "\n",
    "    # get similarity matrix\n",
    "    lengths = jnp.stack([msa_inputs[\"lengths\"], jnp.broadcast_to(x_ref_len,msa_inputs[\"lengths\"].shape)],-1)\n",
    "    sm_mtx = emb @ emb[0,:x_ref_len].T\n",
    "    sm_mask = jnp.broadcast_to(msa_inputs[\"x\"].sum(-1,keepdims=True), sm_mtx.shape)\n",
    "    sm_mtx = nf.norm_row_col(sm_mtx, sm_mask, norm_mode=\"fast\")\n",
    "\n",
    "    # get alignment\n",
    "    aln = sw.sw()(sm_mtx, lengths, msa_params[\"gap\"], 1.0)\n",
    "\n",
    "    # get msa\n",
    "    x_msa = jnp.einsum(\"nia,nij->nja\", msa_inputs[\"x\"], aln)\n",
    "    \n",
    "    # msa loss (push sequences to look similar to query)\n",
    "    x_ref = msa_inputs[\"x\"][0,:x_ref_len,:]\n",
    "    msa_loss = 1.0 - (x_ref * x_msa.mean(0)).sum(-1).mean()\n",
    "\n",
    "    # add gap character\n",
    "    x_msa = x_msa.at[0,:,:].set(msa_inputs[\"x\"][0,:x_ref_len,:])\n",
    "    x_gap = jax.nn.relu(1 - x_msa.sum(-1,keepdims=True))\n",
    "    x_msa_gap = jnp.concatenate([x_msa,jnp.zeros_like(x_gap),x_gap],-1)\n",
    "\n",
    "    # update msa\n",
    "    inputs_mod = inputs\n",
    "    inputs_mod[\"msa_feat\"] = jnp.zeros_like(inputs[\"msa_feat\"]).at[...,0:22].set(x_msa_gap).at[...,25:47].set(x_msa_gap)\n",
    "\n",
    "    # get alphafold ouputs\n",
    "    outputs,_ = model_runner.apply(model_params, key, inputs_mod)\n",
    "    \n",
    "    #################\n",
    "    # compute loss\n",
    "    #################\n",
    "    \n",
    "    # confidence metrics\n",
    "    pae_loss = jax.nn.softmax(outputs[\"predicted_aligned_error\"][\"logits\"])\n",
    "    pae_loss = (pae_loss * jnp.arange(pae_loss.shape[-1])).sum(-1).mean()\n",
    "    plddt_loss = jax.nn.softmax(outputs[\"predicted_lddt\"][\"logits\"])\n",
    "    plddt_loss = (plddt_loss * jnp.arange(plddt_loss.shape[-1])[::-1]).sum(-1).mean()\n",
    "    \n",
    "    # distance to correct solution\n",
    "    rmsd_loss = jnp_rmsd(INPUTS[\"protein_obj\"].atom_positions[:,1,:],\n",
    "                         outputs[\"structure_module\"][\"final_atom_positions\"][:,1,:])\n",
    "\n",
    "    if supervised:\n",
    "      loss = rmsd_loss\n",
    "    else:\n",
    "      loss = pae_loss + plddt_loss + msa_loss\n",
    "      \n",
    "    plddt = 1 - get_lddt_loss(outputs[\"predicted_lddt\"][\"logits\"])    \n",
    "    outs = {\"final_atom_positions\":outputs[\"structure_module\"][\"final_atom_positions\"],\n",
    "            \"final_atom_mask\":outputs[\"structure_module\"][\"final_atom_mask\"]}\n",
    "\n",
    "    return loss, ({\"plddt\": plddt,\n",
    "                   \"rmsd\":rmsd_loss, \"msa_loss\":msa_loss,\n",
    "                   \"outputs\":outs,                   \n",
    "                   \"msa\":x_msa, \"seq\":x_msa[0],\n",
    "                  })\n",
    "  \n",
    "  return mod, jax.value_and_grad(mod, has_aux=True, argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# SETUP ADAM\n",
    "######################\n",
    "\n",
    "# gradient function\n",
    "msa_inputs = {\"x\":INPUTS[\"ms\"], \"aln\":INPUTS[\"aln\"], \"lengths\":INPUTS[\"lens\"]}\n",
    "loss_fn, grad_fn = get_grad_fn(model_runner, x_ref_len=INPUTS[\"lens\"][0])\n",
    "\n",
    "from jax.experimental.optimizers import adam\n",
    "init_fun, update_fun, get_params = adam(step_size=1e-2)\n",
    "\n",
    "@jax.jit\n",
    "def step(i, state, key, inputs, model_params, msa_inputs):\n",
    "  (loss, outs), grad = grad_fn(get_params(state), key, inputs, model_params, msa_inputs)\n",
    "  state = update_fun(i, grad, state)\n",
    "  return state, (loss,outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = []\n",
    "for mode in [\"random\",\"ba\",\"ba_sup\",\"smurf\",\"ba_smurf\",\"ba_sup_smurf\"]: # testing 5 MSA pretraining methods\n",
    "  # random = starting with random convolutions\n",
    "  # ba = pretrain convolutions using basic-align\n",
    "  # ba_sup = pretrain convolutions using basic-align to recreate the initial MSA\n",
    "  # smurf = pretrain convolutions using smurf\n",
    "  # ba_smurf = pretrain convolutions using smurf+basic-align\n",
    "  # ba_sup_smurf = pretrain convolutions using smurf+basic-align\n",
    "\n",
    "  BEST_PLDDT = 0\n",
    "  losses = []\n",
    "  for seed in range(5): # note: in manuscript we ran 20 each\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    ###################################\n",
    "    # pretrain w/ BasicAlign\n",
    "    ###################################\n",
    "    msa_inputs = {\"x\":INPUTS[\"ms\"], \"aln\":INPUTS[\"aln\"], \"lengths\":INPUTS[\"lens\"]}\n",
    "    if mode == \"random\":\n",
    "      state = init_fun({\"emb\":nf.Conv1D_custom()(20,512,18,key=key),\n",
    "                        \"gap\":jnp.full([],-3.0,dtype=jnp.float32)})\n",
    "    if \"ba\" in mode:\n",
    "      if \"sup\" in mode:\n",
    "        msa_model = nf.BasicAlign(X=INPUTS[\"ms\"],lengths=INPUTS[\"lens\"],\n",
    "                                  sw_open=None, sw_gap=-3.0, sw_learn_gap=True,\n",
    "                                  seed=seed, batch_size=INPUTS[\"N\"],\n",
    "                                  supervise=True, nat_aln=INPUTS[\"aln\"])\n",
    "      else:\n",
    "        msa_model = nf.BasicAlign(X=INPUTS[\"ms\"],lengths=INPUTS[\"lens\"],\n",
    "                                  sw_open=None, sw_gap=-3.0, sw_learn_gap=True,\n",
    "                                  seed=seed, batch_size=INPUTS[\"N\"])\n",
    "    if \"smurf\" in mode:\n",
    "      smurf_model = nf.MRF(X=INPUTS[\"ms\"], lengths=INPUTS[\"lens\"], batch_size=INPUTS[\"N\"],\n",
    "                           sw_open=None, sw_gap=-3.0, sw_learn_gap=True, seed=seed)\n",
    "      if \"ba\" in mode:\n",
    "        _ = msa_model.fit(2000, verbose=False)\n",
    "        msa_params = msa_model.opt.get_params()\n",
    "\n",
    "        smurf_params = smrf_model.opt.get_params()\n",
    "        smurf_params.update({k:msa_params[k] for k in [\"emb\",\"gap\",\"open\"]})\n",
    "        smurf_model.opt.set_params(smurf_params)\n",
    "        \n",
    "      _ = smurf_model.fit(2000, verbose=False)\n",
    "      smurf_params = smurf_model.opt.get_params()\n",
    "      state = init_fun({\"emb\":smurf_params[\"emb\"],\"gap\":smurf_params[\"gap\"]})    \n",
    "    \n",
    "    elif \"ba\" in mode:\n",
    "      _ = msa_model.fit(4000, verbose=False)\n",
    "      msa_params = msa_model.opt.get_params()\n",
    "      state = init_fun({\"emb\":msa_params[\"emb\"],\"gap\":msa_params[\"gap\"]})    \n",
    "        \n",
    "    # optimize!\n",
    "    losses.append([])\n",
    "    for i in range(300):\n",
    "      key,subkey = jax.random.split(key)\n",
    "      state,(loss,outs) = step(i, state, subkey, inputs, model_params, msa_inputs)\n",
    "      \n",
    "      # save results\n",
    "      losses[-1].append([float(loss),*[float(outs[x]) for x in [\"plddt\",\"rmsd\",\"msa_loss\"]]])\n",
    "      print(seed, i, *losses[-1][-1])\n",
    "      if outs[\"plddt\"] > BEST_PLDDT:\n",
    "        BEST_PLDDT = outs[\"plddt\"]\n",
    "        save_pdb(outs,f\"{DOM}.REDO3.{mode}.seed{seed}.pred.pdb\")\n",
    "        \n",
    "  LOSSES.append(losses)  \n",
    "  \n",
    "np.save(f\"{DOM}.traj.REDO3.npy\",LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = np.load(f\"{DOM}.traj.REDO3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMS = [\"T1070-D1\",\"T1039-D1\",\"T1038-D1\",\"T1043-D1\",\"T1064-D1\"]\n",
    "VALS = [[0.627021797996172,7.9085627],[0.522569996053579,9.804814],\n",
    "        [0.725891903100609,8.154645],[0.361457601819099,17.92791],\n",
    "        [0.354278629507625,13.859991]]\n",
    "vals = {d:v for d,v in zip(DOMS,VALS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(LOSSES).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4*6),dpi=100)\n",
    "\n",
    "k = 1\n",
    "for losses,nam in zip(LOSSES,[\"random\", \"ba\", \"ba_sup\", \"smurf\", \"ba_smurf\", \"ba_sup_smurf\"]):\n",
    "  losses = np.asarray(losses)\n",
    "  \n",
    "  plt.subplot(6,3,k)\n",
    "  plt.title(f\"{DOM} - {nam}\")\n",
    "  \n",
    "    \n",
    "  plt.plot(losses[:,:,1].T)\n",
    "  if k in [7+9]: plt.xlabel(\"Iterations\");\n",
    "  if k in [1,4,7,1+9,4+9,7+9]: plt.ylabel(\"pLDDT\")\n",
    "  plt.ylim(0.2,1.0)\n",
    "  plt.xlim(0,300)\n",
    "  plt.yticks([0.2,0.4,0.6,0.8,1.0],[0.2,0.4,0.6,0.8,1.0])\n",
    "  plt.plot([0,300],[vals[DOM][0],vals[DOM][0]],color=\"black\")\n",
    "  k += 1\n",
    "  \n",
    "  plt.subplot(6,3,k)\n",
    "  plt.plot(losses[:,:,2].T)\n",
    "  if k in [8+9]: plt.xlabel(\"Iterations\")\n",
    "  plt.ylabel(\"RMSD\")\n",
    "  plt.ylim(1,32)\n",
    "  plt.xlim(0,300)\n",
    "  plt.yscale(\"log\",basey=2);plt.yticks([1,2,4,8,16,32],[1,2,4,8,16,32])\n",
    "  plt.plot([0,300],[vals[DOM][1],vals[DOM][1]],color=\"black\")\n",
    "  k += 1\n",
    "  \n",
    "  plt.subplot(6,3,k)\n",
    "  plt.scatter(losses[:,:,1],\n",
    "              losses[:,:,2],\n",
    "              c=losses[:,:,0],vmin=10,vmax=80,s=10, cmap=\"viridis_r\")\n",
    "  \n",
    "  plt.yscale(\"log\",basey=2,)\n",
    "  plt.yticks([1,2,4,8,16,32],[1,2,4,8,16,32],color=\"tab:red\")\n",
    "  plt.ylim(1,32)\n",
    "  plt.ylabel(\"RMSD\",color=\"tab:red\")\n",
    "\n",
    "  if k in [9]: plt.xlabel(\"pLDDT\",color=\"tab:blue\")\n",
    "  plt.xlim(0.2,1.0)\n",
    "  plt.xticks([0.2,0.4,0.6,0.8,1.0],[0.2,0.4,0.6,0.8,1.0],color=\"tab:blue\")\n",
    "\n",
    "  plt.plot([0.2,1],[vals[DOM][1],vals[DOM][1]],\"--\",color=\"tab:red\")\n",
    "  plt.plot([vals[DOM][0],vals[DOM][0]],[1,32],\"--\",color=\"tab:blue\")\n",
    "\n",
    "  plt.colorbar(label=\"loss\")\n",
    "  k += 1\n",
    "  \n",
    "plt.savefig(f\"{DOM}.traj.REDO3.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES_FLAT = np.concatenate(LOSSES,0)\n",
    "best_idx = LOSSES_FLAT[:,-1,1].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(6,3),dpi=100)\n",
    "\n",
    "ax1.set_title(f\"{DOM}\")\n",
    "\n",
    "ax1.plot(LOSSES_FLAT[best_idx][:,1],color=\"tab:blue\")\n",
    "ax1.set_ylabel(\"pLDDT\",color=\"tab:blue\")\n",
    "ax1.set_ylim(0.2,1.0)\n",
    "ax1.set_yticks([0.2,0.4,0.6,0.8,1.0])\n",
    "ax1.set_yticklabels([0.2,0.4,0.6,0.8,1.0],color=\"tab:blue\")\n",
    "ax1.set_xlim(0,300)\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax1.plot([0,300],[vals[DOM][0],vals[DOM][0]],\"--\",color=\"tab:blue\")\n",
    "\n",
    "\n",
    "ax1_ = ax1.twinx()\n",
    "ax1_.plot(LOSSES_FLAT[best_idx][:,2],color=\"tab:red\")\n",
    "ax1_.plot([0,300],[vals[DOM][1],vals[DOM][1]],\"--\",color=\"tab:red\")\n",
    "ax1_.axis(\"off\")\n",
    "\n",
    "ax2.get_shared_y_axes().join(ax2, ax1_)\n",
    "s = ax2.scatter(LOSSES_FLAT[...,1],\n",
    "                LOSSES_FLAT[...,2],\n",
    "                c=LOSSES_FLAT[...,0],vmin=10,vmax=80,s=10, cmap=\"viridis_r\")\n",
    "\n",
    "fig.colorbar(s, ax=ax2, label=\"Loss\")\n",
    "\n",
    "ax2.set_yscale(\"log\",basey=2,)\n",
    "ax2.set_yticks([1,2,4,8,16,32])\n",
    "ax2.set_yticklabels([1,2,4,8,16,32],color=\"tab:red\")\n",
    "ax2.set_ylim(1,32)\n",
    "#ax2.set_ylabel(\"RMSD\",color=\"tab:red\")\n",
    "\n",
    "ax2.set_xlabel(\"pLDDT\",color=\"tab:blue\")\n",
    "ax2.set_xlim(0.2,1.0)\n",
    "ax2.set_xticks([0.2,0.4,0.6,0.8,1.0])\n",
    "ax2.set_xticklabels([0.2,0.4,0.6,0.8,1.0],color=\"tab:blue\")\n",
    "\n",
    "ax2.plot([0.2,1],[vals[DOM][1],vals[DOM][1]],\"--\",color=\"tab:red\")\n",
    "ax2.plot([vals[DOM][0],vals[DOM][0]],[1,32],\"--\",color=\"tab:blue\")\n",
    "\n",
    "fig.savefig(f\"{DOM}.traj.REDO3.best.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
